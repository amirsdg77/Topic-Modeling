# -*- coding: utf-8 -*-
"""Topic Modeling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cd3dZNn3K8D5Xobxp6nhIXwKGuymQjqj
"""

!pip install python-bidi
!pip install arabic_reshaper
!pip -q install hazm
!pip -q install clean-text[gpl]
!pip install bs4

from google.colab import files
uploaded = files.upload()



"""# Data Preprocessing"""

from pandas import *
from pandas import ExcelFile
import pandas as pd

#import ExcelFile
df = pd.read_csv('labeled comments up to 20211017 1109.csv')
# df = x.parse(x.sheet_names[0])

stop_words = open('StopWords.txt').read()
StopWords = stop_words.split("\n")

import arabic_reshaper
from hazm import Normalizer
from hazm import WordTokenizer
from cleantext import clean
from bs4 import BeautifulSoup
import re

tokenizer = WordTokenizer(join_verb_parts=False,replace_hashtags=False, replace_IDs=False)
normalizer = Normalizer(remove_extra_spaces=True, persian_numbers=False, persian_style=True,
                        punctuation_spacing=False, remove_diacritics=True,
                        affix_spacing=False, token_based=True)

import nltk
nltk.download('punkt')
nltk.download('wordnet')

import nltk
from hazm import Stemmer, Lemmatizer
from nltk import word_tokenize, sent_tokenize
# from PersianStemmer import PersianStemmer
from nltk.stem import WordNetLemmatizer
import string

from cleantext import clean
import codecs,re
def arabic_to_farsi(text):
    text = re.sub(r'[Ùƒï®‘ï®ï®ï®Žï»œï»›ï»šï»™]', r'Ú©', text)
    text = re.sub(r'[Ù‰Ù‰Ù‰ï»´ï»¢ï»³ï»²ï»±ï»°Ù‰Ù‰ï»¯ÙŠ]', r'ÛŒ', text)
    return text

def cleaning(text):
  text = str(text)
  text = re.sub(r'#', ' ', text)
  text = re.sub(r'_', ' ', text)
  text = text.replace('\u200c', ' ')
  text = re.sub(r'([Ø§-ÛŒ])\1{2,}', r'\1', text) # bbb+ -> b at least 2 char
  text = re.sub(r'[^\w\s]',' ',text)
  text = re.sub(r'\@\w*', ' ', text)
  text = re.sub(r'\d+', ' ', text)
  text = re.sub(r'$', ' ', text)
  text = re.sub(r"[a-zA-Z0-9]+", " ",text)
  text = re.sub(r'<(.*?)>', ' ', text)
  text = re.sub(r'https?:\/\/.*[\r\n]*', ' ', text)
  text=re.sub(r'[Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹Ù Ù¡Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù§Ù¨Ù¨Ù©Ù©â—]',' ',text)
  text=re.sub(r'[\Â·\â™¦\Ù­\\,\^\|\Ë\Ù¬\â€™\â€\â€¹\â–ª\â—‹Â¼Ã§Â½Ã©ÃªÃ¼É™Ä±Å“â„¢ÃÃ¡Ã Ã¤ÄÄ‡Ã­Å‚Ã±Å«Â©Ù”Ù°]',' ',text)
  text=re.sub(r'[Ù‘ÙŽÙÙÙ‹ÙÙŒÙ’]',' ',text)
  text=re.sub(r'[ï€­ï¶ïƒ¼]',r' ',text)
  text=re.sub(r'[\â€“\â€”â€¦Â°â‰ˆâ‰ Â±â‰¤â‰¥\âˆ’Ã—Ã·âˆšÙªâ†’â†â†”â†‘â†“\#\Ù«]',u' ',text) 
  text=text.replace(u'â€¢',u' ').replace(u'Ëˆ',u' ').replace(u'Ø›',u' ').replace(u'/',u' ').replace(u'Û€',u'Ù‡Ù”').replace(u"ï´¿",u' ').replace(u"ï´¾",u' ').replace(u"'",u' ').replace(u'\\',u' ').replace(u'[',u' ').replace(u']',u' ').replace(u'?',u' ').replace(u'ØŸ',u' ').replace(u')',u' ').replace(u'_',u' ').replace(u'(u',u' ').replace(u'}',u' ').replace(u'{',u' ').replace(u'.',u' ').replace(u'>',u' ').replace(u'<',u' ')
  text=text.replace(u'`',u' ').replace(u'\t',u' ').replace(u'=',u' ').replace(u'Â»',u' ').replace(u'Â«',u' ').replace(u'~',u' ').replace(u'!',u' ').replace(u'@',u' ').replace(u'$',u' ').replace(u',u',u' ').replace(u'%',u' ').replace(u'ØŒ',u' ').replace(u'-',u' ').replace(u';',u' ').replace(u':',u' ').replace(u'*',u' ').replace(u'"',u' ').replace(u'&',u' ').replace(u'#',u' ').replace(u'+',u' ')
  text=re.sub(r'[\n\r]{2,}',u'\n',text)
  text = re.sub(u'(\u202A|\u202B|\u202C|\u202D|\u202E|\u200F|\uFEFF|\u2003|\Â¬|\Â­)',u'\u200C', text)#Ø­Ø°Ù Ú©Ø§Ø±Ú©ØªØ±Ù‡Ø§ÛŒ ØªØºÛŒÛŒØ±Ø¬Ù‡Øª
  text = re.sub(u'â€Œ{2,}', u'â€Œ', text) # Ù¾Ø´Øªâ€ŒØ³Ø±Ù‡Ù…
  text = re.sub(u'â€Œ(?![Ø¦Ø§Ø¢Ø£Ø¥Ú˜Ø²Ø±Ø°Ø¯ÙˆØ¤Ø©Ø¨Ù¾ØªØ«Ø¬Ú†Ø­Ø®Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚Ú©Ú¯Ù„Ù…Ù†Ù‡ÛŒÙŠÙ‹ÙŒÙÙŽÙÙÙ‘Ù’Ù°Ù“Ù”]|[\u0900-\u097F]|Ö¹)', u'', text) # Ø¯Ø± Ù¾Ø³
  text = re.sub(u'(?<![Ø¦Ø¨Ù¾ØªØ«Ø¬Ú†Ø­Ø®Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚Ú©Ú¯Ù„Ù…Ù†Ù‡ÛŒÙŠÙ‹ÙŒÙÙŽÙÙÙ‘Ù’Ù°Ù“Ù”]|[\u0900-\u097F]|f|Ö¹)â€Œ', u'', text) # Ø¯Ø± Ù¾ÛŒØ´
  text=text.replace(u'Â­',u' ').replace(u'Â­',u' ').replace(u'Ù€',u' ').replace(u'Ù€',u' ').replace(u'Ù€',u' ').replace(u'Ù€',u' ').replace(u'',u' ')
  text=text.replace(u'',u' ')
  text = re.sub(u'â€Œ{2,}', u'â€Œ', text) # Ù¾Ø´Øªâ€ŒØ³Ø±Ù‡Ù…
  text = re.sub(u'(\u00A0)',u' ', text).replace(u'(',u' ').replace(u')',u' ')
  text=text.replace(u'    ',u' ').replace(u'    ',u' ').replace(u'   ',u' ').replace(u'  ',u' ').replace(u'  ',u' ').replace(u'  ',u' ')
  text=arabic_to_farsi(text)
  return text.strip()

def remove_stopwords(text):
  tokens = word_tokenize(text)
  persian_stopwords = StopWords

  results = [w for w in tokens if w not in persian_stopwords and w not in string.punctuation]
  
  return ' '.join(results)

def stemming(text):
  tokens = word_tokenize(text)
  persian_stemmer = Stemmer()

  results = [persian_stemmer.stem(w) for w in tokens]

  return ' '.join(results)

def lemmatizing(text):
  wordlist = []
  lemmatizer = WordNetLemmatizer()
  
  sentences = sent_tokenize(text)

  for sentence in sentences:
        words=word_tokenize(sentence)
        for word in words:
            wordlist.append(lemmatizer.lemmatize(word))

  return ' '.join(wordlist)

def normalize(text):
  result = lemmatizing(text)
  result = stemming(text)
  result = remove_stopwords(result)

  return result

for i in range(len(df)):
  df.Comment[i] = str(df.Comment[i]).replace(df.BrandName[i],'')

df['Clean_Body'] = df.Comment.apply(cleaning)
df['normalized'] = df.Clean_Body.apply(normalize)

df

"""# Document Term Matrix"""

import pandas as pd

sentences = df['normalized']

all_words = []
for sentence in sentences:
  all_words += word_tokenize(sentence)
all_words = list(dict.fromkeys(all_words))

all_valid_words = [x for x in all_words if len(x) != 1]

matrix = [[0 for i in range(len(all_words))] for j in range(len(sentences))]

for i in range(len(sentences)):
  tokenized_sent = word_tokenize(sentences[i])
  for j in range(len(all_words)):
    if all_words[j] in tokenized_sent:
      matrix[i][j] = 1

data = pd.DataFrame(matrix, columns=all_words)

data

"""# LDA"""

doc_clean = [word_tokenize(x) for x in df['normalized']]

import gensim
from gensim import corpora

# Creating the term dictionary of our courpus, where every unique term is assigned an index. 
dictionary = corpora.Dictionary(doc_clean)

# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.
doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]

Lda = gensim.models.ldamodel.LdaModel
ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)

print(ldamodel.print_topics(num_topics=3, num_words=3))

"""# LSA"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.pipeline import Pipeline
documents = [x for x in df['normalized']]  
# raw documents to tf-idf matrix: 
vectorizer = TfidfVectorizer(stop_words='english', 
                             use_idf=True, 
                             smooth_idf=True)
# SVD to reduce dimensionality: 
svd_model = TruncatedSVD(n_components=100,
                         algorithm='randomized',
                         n_iter=10)
# pipeline of tf-idf + SVD, fit to and applied to documents:
svd_transformer = Pipeline([('tfidf', vectorizer), 
                            ('svd', svd_model)])
svd_matrix = svd_transformer.fit_transform(documents)

svd_matrix

"""# Word and Sentence Embeddings"""

import nltk
nltk.download('punkt')
import numpy as np
import pandas as pd

sentences = [x for x in df['normalized']]

tokenized_sent = []
for s in sentences:
    tokenized_sent.append(word_tokenize(s.lower()))

def cosine(u, v):
    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))

"""## Doc2vec"""

from gensim.models.doc2vec import Doc2Vec, TaggedDocument
def sentence_embedding(tokenized_sent):
  tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_sent)]
  model = Doc2Vec(tagged_data, vector_size = 20, window = 2, min_count = 1, epochs = 100)
  model.wv.vocab

  return model

# test
# test_doc = word_tokenize("Ø³Ù„Ø§Ù… Ù…Ù† Ø­Ø§Ù„Ù… Ø®ÙˆØ¨ Ø§Ø³Øª".lower())
# test_doc_vector = model.infer_vector(test_doc)
# model.docvecs.most_similar(positive = [test_doc_vector])

"""## Word2vec"""

from gensim.models import Word2Vec
def word_embedding(sentences):
  a = [word_tokenize(x) for x in sentences]
  model = Word2Vec(sentences=a, window=5, min_count=1, workers=4)

  return model

def compare_embeddings(word, tokenized_sent, sentences):
  word_model = word_embedding(sentences)
  sentence_model = sentence_embedding(tokenized_sent)

  print(f"word2vec:\n {word}:{word_model.wv.most_similar(word, topn=5)}")
  print(f"doc2vec:\n {word}:{sentence_model.wv.most_similar(word, topn=5)}")

compare_embeddings("Ø³Ù„Ø§Ù…", tokenized_sent, sentences)